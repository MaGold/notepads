{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 4\n",
    "##Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "\n",
    "The null space of $A$ and the row space of $A$ are orthogonal subspaces.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "$$Ax = \\begin{pmatrix} - &\\text{row } 1& - \\\\ &\\vdots& \\\\ - &\\text{row }m &- \\end{pmatrix} \\begin{pmatrix} \\\\ x \\\\ \\\\ \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix},$$\n",
    "\n",
    "so every row has a $0$ dot product with $x$.\n",
    "\n",
    "**Corollary**:\n",
    "\n",
    "The null space of $A^T$ and the column space of $A$ are orthogonal subspaces.\n",
    "\n",
    "The **orthogonal complement** ($V^\\perp$, \"V perp\") of $V$ containes every vector that is orthogonal to $V$.\n",
    "\n",
    "**Fundamental Theorem of Linear Algebra (Part 2)**:\n",
    "\n",
    "- The nullspace is the orthgonal complement of the row space in ($\\mathbf{R}^n$).\n",
    "- The left nullspace is the orthogonal complement of the column space (in $\\mathbf{R}^m$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma**:\n",
    "\n",
    "$\\mathbf{A}^T\\mathbf{A}$ is invertible if and only if $\\mathbf{A}$ has linearly independent columns.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "Linearly independent columns $\\iff \\text{Null}(\\mathbf{A}) = \\{\\mathbf{0}\\}.$\n",
    "Since a square matrix is invertible if and only if its nullspace contains only $\\mathbf{0}$, it is enough to show that $\\text{Null}(\\mathbf{A}) = \\text{Null}(\\mathbf{A}^T\\mathbf{A})$.\n",
    "\n",
    "If $\\mathbf{A} \\mathbf{x} = \\mathbf{0}$ then $\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{0}$.\n",
    "\n",
    "Conversely, $\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{0} \\implies \\mathbf{x}^T \\mathbf{A}^T \\mathbf{A} \\mathbf{x} = 0 \\implies (\\mathbf{A} \\mathbf{x})^T \\mathbf{A} \\mathbf{x} =0 \\implies ||\\mathbf{Ax}||^2 = 0 \\implies \\mathbf{Ax} = \\mathbf{0}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Projections\n",
    "\n",
    "### Projection onto a line\n",
    "\n",
    "Given vectors $\\mathbf{a}$ and $\\mathbf{b}$, find the point $\\mathbf{p}$ along the line $\\mathbf{a}$ that is closest to $\\mathbf{b}$. The vector $\\mathbf{p}$ is the **projection of $\\mathbf{b}$ on $\\mathbf{a}$**, it is some multiple of $\\mathbf{a}$. So $\\mathbf{p} = \\hat{x} \\mathbf{a}$.\n",
    "\n",
    "We want $\\mathbf{a} \\cdot (\\mathbf{b} - \\hat{x}\\mathbf{a}) = 0$, so\n",
    "\n",
    "$$\\hat{x} = \\frac{\\mathbf{a}^T\\mathbf{b}}{\\mathbf{a}^T\\mathbf{a}}$$\n",
    "\n",
    "and since $\\mathbf{p} = \\mathbf{a} \\hat{x} = \\mathbf{a} \\frac{\\mathbf{a}^T\\mathbf{b}}{\\mathbf{a}^T\\mathbf{a}} = \\mathbf{Pb}$, the **projection matrix** is $$\\mathbf{P} = \\frac{\\mathbf{a}\\mathbf{a}^T}{\\mathbf{a}^T\\mathbf{a}}.$$\n",
    "\n",
    "The numerator is a matrix (a column $\\mathbf{a}$ times a row $\\mathbf{a}^T$), the denominator is a number. $\\mathbf{P}$ is a rank one ($m \\times m$) matrix (we are projection onto a one-dimensional subspace).\n",
    "\n",
    "### Projection onto a subspace\n",
    "\n",
    "Given $n$ linearly independent vectors $\\mathbf{a_1}, \\ldots, \\mathbf{a_n}$ in $\\mathbf{R}^m$ and a vector $\\mathbf{b}$ in $\\mathbf{R}^m$, find the combination $\\hat{x_1}\\mathbf{a_1} + \\cdots + \\hat{x_n}\\mathbf{a_n}$ that is closest to $\\mathbf{b}$.\n",
    "\n",
    "We are projecting $\\mathbf{b}$ in $\\mathbf{R}^m$ onto the subspace spanned by the $\\mathbf{a}$s. I.e. we are look for the particular combination $\\mathbf{p} = \\mathbf{A}\\mathbf{\\hat{x}}$ that is closest to $\\mathbf{b}$, where the columns of $A$ are the column vectors $\\mathbf{a_1}, \\ldots, \\mathbf{a_n}$.\n",
    "\n",
    "We want $\\mathbf{A}^T(\\mathbf{b} - \\mathbf{A \\hat{x}}) = \\mathbf{0}$, or $\\mathbf{A}^T\\mathbf{A}\\mathbf{\\hat{x}} = \\mathbf{A}^T\\mathbf{b}$. The matrix $\\mathbf{A}^T\\mathbf{A}$ is symmetric, and it is invertible since the $\\mathbf{a}$s are independent, so\n",
    "\n",
    "$$\\mathbf{\\hat{x}} = {\\left(\\mathbf{A}^T\\mathbf{A}\\right)}^{-1} \\mathbf{A}^T \\mathbf{b}$$\n",
    "\n",
    "and so the projection of $\\mathbf{b}$ on the subspace is the vector\n",
    "$\\mathbf{p} = \\mathbf{A \\hat{x}} = \\mathbf{A}{\\left(\\mathbf{A}^T\\mathbf{A}\\right)}^{-1} \\mathbf{A}^T \\mathbf{b}$, and the **projection matrix** is\n",
    "\n",
    "$$\\mathbf{P} = \\mathbf{A}{\\left(\\mathbf{A}^T\\mathbf{A}\\right)}^{-1} \\mathbf{A}^T.$$\n",
    "\n",
    "Note: the projection matrix $\\mathbf{P}$ satisfies\n",
    "- $\\mathbf{P} = \\mathbf{P}^T$;\n",
    "- $\\mathbf{P}^2 = \\mathbf{P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 5\n",
    "## Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intro\n",
    "If $x \\ne0$ and $A x = \\lambda x$ then $x$ is an *eigenvector* and $\\lambda$ is an *eigenvalue*.\n",
    "\n",
    "$A x = \\lambda x  \\iff (A - \\lambda I) x = 0  \\iff (A - \\lambda) I$ not invertible (singular)  $\\iff \\det(A - \\lambda I)=0$\n",
    "\n",
    "Want to check your calculations?\n",
    "- $\\sum \\lambda_i = trace(A)$\n",
    "- $\\prod \\lambda_i = \\det(A)$\n",
    "\n",
    "Note: if 0 is an eigenvalue, then A is not invertible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Diagonolization\n",
    "   \n",
    "**Theorem**:\n",
    "\n",
    "Suppose $A$ is an $n \\times n$ matrix that has n linearly independent eigenvectors $x_1, \\ldots, x_n$. Let $\\lambda_1, \\ldots, \\lambda_n$ be the corresponding eigenvalues.\n",
    "Let \n",
    "\n",
    "$$S = \\begin{pmatrix} | & | & | & | \\\\ x_1 & x_2 & ... & x_n \\\\ | & | & | & |  \\end{pmatrix}$$\n",
    "and\n",
    "$$\\Lambda = \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0  & \\ddots & 0 \\\\ 0 & 0 & \\lambda_n\\end{pmatrix}$$.\n",
    "\n",
    "Then $A$ can be diagonalized as $$A = S \\Lambda S^{-1}.$$\n",
    "   \n",
    "**Proof**: \n",
    "\n",
    "$$S^{-1} A S = S^{-1} \\begin{pmatrix} | & | & | & | \\\\ \\lambda_1 x_1 & \\lambda_2 x_2 & ... & \\lambda_n x_n \\\\ | & | & | & |  \\end{pmatrix} = S^{-1} \\begin{pmatrix} | & | & | & | \\\\ x_1 & x_2 & ... & x_n \\\\ | & | & | & |  \\end{pmatrix}  \\begin{pmatrix} \\lambda_1 & 0 & 0 \\\\ 0  & \\ddots & 0 \\\\ 0 & 0 & \\lambda_n\\end{pmatrix}  = S^{-1} S \\Lambda = \\Lambda$$\n",
    " \n",
    "Notes:\n",
    "   - $A$ and $\\Lambda$ have the same eivenvalues: $S \\Lambda S^{-1} x = A x = \\lambda x \\Rightarrow \\Lambda S^{-1} x = \\lambda S^{-1} x$ (they may have different eigenvectors)\n",
    "   - $n$ eiganvalues not all distinct $\\Rightarrow$ eigenvectors not necessarily linearly independent $\\Rightarrow S$ not necessarily invertible $\\Rightarrow$ might not be able to diagonalize\n",
    "\n",
    "####Theorem: \n",
    "Eigenvectors $x_1, ..., x_j$ that correspond to distinct eigenvalues are linearly independent. (An $n x n$ matrix with n different eigenvalues is diagonalizable.)\n",
    "\n",
    "####Proof: For n=2: \n",
    "   - Suppose $c_1 x_1 + c_2 x_2  = 0$\n",
    "   - Multiply by $A: c_1 \\lambda_1 x_1 + c_2 \\lambda_2 x_2 = 0$.\n",
    "   - Multiply by $\\lambda_2: c_1 \\lambda_2 x_1 + c_2 \\lambda_2 x_2 = 0$.\n",
    "   - So, $(\\lambda_1-\\lambda_2) c_1 x_1 = 0$ and this implies $c_1 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Symmetric Matrices\n",
    "   \n",
    "#####Lemma:\n",
    "The eigenvalues of a real symmetric matrix are real.\n",
    "\n",
    "#####Proof: \n",
    "   - $A x = \\lambda x \\Rightarrow A \\overline{x} = \\overline{\\lambda} \\overline{x} \\Rightarrow \\overline{x}^T A = \\overline{x}^T \\overline{\\lambda}$.\n",
    "   - Therefore, $\\overline{x}^T A x = \\overline{x}^T \\lambda x$ and $\\overline{x}^T A x = \\overline{x}^T \\overline{\\lambda} x$.\n",
    "   - Since $\\lambda \\overline{x}^T x = \\overline{\\lambda} \\overline{x}^T x$ and $\\overline{x}^T x \\ne 0$,  we get $\\lambda = \\overline{\\lambda}$.\n",
    "   \n",
    "#####Lemma:\n",
    "The eigenvectors of a real symmetric matrix are orthogonal.\n",
    "\n",
    "#####Proof:\n",
    "Suppose $Ax = \\lambda_1x$ and  $Ay = \\lambda_2y$. Then $(\\lambda_1x)^Ty = {(Ax)}^Ty = x^TA^Ty = x^TAy = x^T\\lambda_2y.$ Therefore, $x^T \\lambda_1 y = x^T \\lambda_2 y$. Since $\\lambda_1 \\ne \\lambda_2$, we must have $x^Ty = 0$.\n",
    "\n",
    "####Theorem:\n",
    "If $A$ is a real symmetric matrix (with no repeated eigenvalues), then $A$ can be factorized as $A = Q \\Lambda Q^T$ with real eigenvalues in $\\Lambda$ and orthgonal eigenvectors in $Q$:\n",
    "   - $A = Q \\Lambda Q^{-1} = Q \\Lambda Q^T (i.e. Q^{-1} = Q^T)$\n",
    "   \n",
    "In fact, the condition of repeated eigenvalues can be dropped, giving the **Spectral Theorem** (for symmetric matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Positive Definite Matrices\n",
    "\n",
    "$A$ is **positive definite** if $x^TAx > 0 $ for every nonzero vector.\n",
    "\n",
    "For a symmetric matrix, the following are equivalent:\n",
    "- All $n$ eigenvalues are postivie.\n",
    "- All $n$ upper left determinants are positive.\n",
    "- All $n$ pivots are positive.\n",
    "- The matrix is *positive definite*.\n",
    "\n",
    "If $A = Q \\Lambda Q^T$ is positive definite, then $x^TAX = 1$ is an ellipse:\n",
    "- $\\begin{pmatrix} x & y \\end{pmatrix} Q\\Lambda Q^T \\begin{pmatrix} x \\\\ y \\end{pmatrix} =  \\begin{pmatrix} X & Y \\end{pmatrix}\\Lambda \\begin{pmatrix} X \\\\ Y \\end{pmatrix} = \\lambda_1 X^2 + \\lambda_2 Y^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Similar Matrices\n",
    "\n",
    "Let $M$ be an invertible matrix. Then $B = M^{-1}AM$ is **similar** to $A$.\n",
    "\n",
    "**Theorem**:\n",
    "\n",
    "Similar matrices have the same eigenvalues.\n",
    "\n",
    "i.e. For every $A$ and invertible $M$,  we have that $A$ and $M^{-1}AM$ have the same eigenvalues.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "Consider $A$ and $B$, where $B=M^{-1}AM$. Then $Ax = \\lambda x \\implies MBM^{-1}x = \\lambda x \\implies BM^{-1}x = \\lambda M^{-1} x$. So $\\lambda$ is also an eigenvalue of $B$ (with eigenvector $M^{-1}x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues:\n",
      "[ 1.82488393 -0.20006987 -0.52555041]\n",
      "[ 1.82488393 -0.20006987 -0.52555041]\n",
      "eigenvectors:\n",
      "[[-0.36668206 -0.83433644 -0.85562388]\n",
      " [-0.75151816  0.54908526  0.14115145]\n",
      " [-0.5484202   0.04886796  0.49797997]]\n",
      "[[-0.58421055  0.55441141 -0.67068299]\n",
      " [ 0.70213537 -0.78373066  0.35838211]\n",
      " [ 0.40706752 -0.27998972  0.64942019]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.rand(3,3)\n",
    "M = np.random.rand(3,3)\n",
    "\n",
    "# B = M^{-1} A M\n",
    "B = np.dot(np.dot(np.linalg.inv(M), A), M)\n",
    "#print(B)\n",
    "\n",
    "print(\"eigenvalues:\")\n",
    "print(np.linalg.eig(A)[0])\n",
    "print(np.linalg.eig(B)[0])\n",
    "    \n",
    "print(\"eigenvectors:\")\n",
    "print(np.linalg.eig(A)[1])\n",
    "print(np.linalg.eig(B)[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Jordan Form\n",
    "\n",
    "####Theorem:\n",
    "\n",
    "If $A$ has $s$ independent eigenvectors, it is simlar to a matrix that has $s$ Jordan blocks on its digagonal:\n",
    "\n",
    "$\\exists M$ such that\n",
    "\n",
    "$$M^{-1} A M = \\begin{pmatrix} J_1 & & \\\\ & \\ddots & \\\\ & & J_s\\end{pmatrix} = J,$$\n",
    "\n",
    "where each block $J$ has one eigenvalue $\\lambda_i$, one eigenvector, and $1$s above the diagonal:\n",
    "\n",
    "$$J_i = \\begin{pmatrix} \\lambda_i & 1 & & \\\\ & \\ddots & \\ddots & \\\\ & & \\ddots & 1 \\\\ & & & \\lambda_i\\end{pmatrix}.$$\n",
    "\n",
    "$A$ and $B$ are similar if and only if they share the same Jordan form $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Singular Value Decomposition\n",
    "\n",
    "The **singular value decomposition** of $A$ consists of an orthogonal matrix $U$, a diagonal matrix $\\Sigma$, and an orthogonal matrix $V$, \n",
    "\n",
    "$$AV = U \\Sigma$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$A = U \\Sigma V^{-1} = U \\Sigma V^T.$$\n",
    "\n",
    "Note: \n",
    "\n",
    "$$A^TA = (V \\Sigma^T U^T )(U \\Sigma V^T) = V \\Sigma^2 V^T,$$ \n",
    "\n",
    "so $V$ has the eigenvectors of $A^TA$ as its columns, and $\\Sigma^2$ has the eigenvalues of $A^TA$. This is how we can find $V$ and $\\Sigma$. Then $U = AV \\Sigma^{-1}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Singular Value Decomposition\n",
    "\n",
    "Let $A$ be a real ($m \\times n$) matrix. There exist\n",
    "- $U$, an orthogonal $(m \\times m)$ matrix where columns = eigenvectors $A^TA$\n",
    "- $\\Sigma$, $(m \\times n)$ consisting of all $0$s, and the square root of the eigenvalues of both $AA^T$ and $A^TA$ on its diagonal\n",
    "- $V$, an orthogonal $(n \\times n)$ matrix where columns = eigenvectors $AA^T$\n",
    "and\n",
    "\n",
    "$$A = U \\Sigma V^T = \\begin{pmatrix} | & | & | & | \\\\ u_1 & u_2 & ... & u_m \\\\ | & | & | & |  \\end{pmatrix} \\begin{pmatrix} \\sigma_1 &  &  &  \\\\  & \\sigma_2 & & \\\\  & & \\ddots & \\\\  &  & & \\sigma_n \\\\  &  & & \\\\  &  & & \\\\  &  & &\\end{pmatrix} \\begin{pmatrix} | & | & | & | \\\\ v_1 & v_2 & ... & v_n \\\\ | & | & | & |  \\end{pmatrix}^T$$\n",
    "\n",
    "The diagonal entries $\\sigma_1, \\sigma_2, \\cdots$ of $\\Sigma$ are the **singular values** of $A$.\n",
    "\n",
    "\n",
    "Note: \n",
    "\n",
    "$$A^TA = (V \\Sigma^T U^T )(U \\Sigma V^T) = V \\Sigma^2 V^T,$$ \n",
    "\n",
    "so $V$ has the eigenvectors of $A^TA$ as its columns, and $\\Sigma^2$ has the eigenvalues of $A^TA$. This is how we can find $V$ and $\\Sigma$. Then $U = AV \\Sigma^{-1}$.\n",
    "\n",
    "**Proof**:\n",
    "\n",
    "Let $v_1, \\ldots, v_n$ be orthonormal eigenvectors of $A^TA$. We have $$A^TAv_i = \\sigma^2 v_i \\implies v_i^TA^TAv_i = \\sigma_i^2 v_i^Tv_i \\implies ||Av_i||^2 = \\sigma_i^2 \\implies ||Av_i|| = \\sigma_i,$$\n",
    "so $$AA^TAv_i = \\sigma_i^2 Av_i \\implies u_i = Av_i/\\sigma_i,$$\n",
    "and therefore $u_i$ is a unit eigenvector of $AA^T$.\n",
    "\n",
    "Note: $A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + \\cdots$, so keeping only the few few largest $\\sigma$, this gives us a compressed approximation of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0.52146314  0.18460203]\n",
      " [ 0.25781966  0.84768995]\n",
      " [ 0.88248687  0.61643618]\n",
      " [ 0.15421777  0.89730469]]\n",
      "V^T:\n",
      " [[-0.82795709  0.56079145]\n",
      " [-0.56079145 -0.82795709]]\n",
      "Sigma:\n",
      " [[ 0.46554381  0.        ]\n",
      " [ 0.          2.61322261]]\n",
      "U:\n",
      " [[-0.70503756 -0.17039292]\n",
      " [ 0.56259724 -0.32390427]\n",
      " [-0.82692352 -0.38468739]\n",
      " [ 0.80661604 -0.31739117]]\n",
      "SVD:\n",
      " [[ 0.52146314  0.18460203]\n",
      " [ 0.25781966  0.84768995]\n",
      " [ 0.88248687  0.61643618]\n",
      " [ 0.15421777  0.89730469]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.random.rand(4,2)\n",
    "print(\"A:\\n\", A)\n",
    "\n",
    "Asymm = np.dot(A.T, A)\n",
    "\n",
    "V = np.linalg.eig(Asymm)[1]\n",
    "print(\"V^T:\\n\", V.T)\n",
    "\n",
    "sigma = np.zeros((2,2))\n",
    "np.fill_diagonal(sigma, np.linalg.eig(Asymm)[0], )\n",
    "print(\"Sigma:\\n\", sigma)\n",
    "\n",
    "\n",
    "U = np.dot(np.dot(A, V), np.linalg.inv(sigma))\n",
    "print(\"U:\\n\", U)\n",
    "\n",
    "svd = np.dot(np.dot(U, sigma), V.T)\n",
    "print(\"SVD:\\n\", svd)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
